{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "vocabulary = read_data(\"text8.zip\")\n",
    "print('Data size', len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
    "                                                            vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Build and train a skip-gram model.\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "  # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "  with tf.device('/cpu:0'):\n",
    "    # Look up embeddings for inputs.\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Compute the average NCE loss for the batch.\n",
    "  # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "  # time we evaluate the loss.\n",
    "  loss = tf.reduce_mean(\n",
    "      tf.nn.nce_loss(weights=nce_weights,\n",
    "                     biases=nce_biases,\n",
    "                     labels=train_labels,\n",
    "                     inputs=embed,\n",
    "                     num_sampled=num_sampled,\n",
    "                     num_classes=vocabulary_size))\n",
    "\n",
    "  # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "  # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(\n",
    "      normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(\n",
    "      valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "  # Add variable initializer.\n",
    "  init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  285.415283203\n",
      "Nearest to from: leh, gemological, lights, jujutsu, cages, bride, dives, confidence,\n",
      "Nearest to into: musically, occlusion, pathologist, grothendieck, by, mcneil, lud, fredric,\n",
      "Nearest to been: illustrators, judson, overshadowed, doubt, musab, hellenism, roaring, sato,\n",
      "Nearest to and: maududi, zoot, poker, posed, benz, halfwidth, solvent, broken,\n",
      "Nearest to which: devoted, mackenzie, fullwidth, originality, separable, plaintexts, churches, icsu,\n",
      "Nearest to while: woolwich, disguise, dayton, metrical, pond, neutrino, especially, pottery,\n",
      "Nearest to between: whistleblower, bushman, thriller, distinguishing, genotype, uruguayan, trombone, emulation,\n",
      "Nearest to he: seriousness, grantley, apsu, raiding, hani, troughs, unforgettable, indoctrination,\n",
      "Nearest to if: bryson, awe, dominion, heathland, demographics, negating, adhesion, cannabis,\n",
      "Nearest to to: encompassed, axel, jacopo, mr, tain, poetic, compute, necromancers,\n",
      "Nearest to were: horsemanship, safely, venetians, barra, warheads, sorcerers, vassar, compression,\n",
      "Nearest to more: anteater, leben, uttering, emperors, advising, blackface, permitted, roebuck,\n",
      "Nearest to people: niggers, crete, harassment, lenition, jurist, circling, bertrand, increasingly,\n",
      "Nearest to six: sociopolitical, sangha, youngster, accorded, ionized, entropy, peres, titan,\n",
      "Nearest to when: wager, accomplished, jolson, lead, examiners, glaucus, pomeranian, invoked,\n",
      "Nearest to for: headlining, confessors, webern, vy, euphemism, kirghiz, conquered, backfield,\n",
      "Average loss at step  20000 :  29.3341755066\n",
      "Average loss at step  40000 :  6.11049232572\n",
      "Average loss at step  60000 :  5.13965054524\n",
      "Average loss at step  80000 :  4.82088492196\n",
      "Average loss at step  100000 :  4.71097334914\n",
      "Average loss at step  120000 :  4.5790661846\n",
      "Average loss at step  140000 :  4.50702497419\n",
      "Average loss at step  160000 :  4.47495551547\n",
      "Average loss at step  180000 :  4.44545509901\n",
      "Average loss at step  200000 :  4.38291290536\n",
      "Nearest to from: through, during, into, in, under, after, roshan, hyi,\n",
      "Nearest to into: through, from, under, affectionate, tempest, elbl, between, within,\n",
      "Nearest to been: be, become, was, were, had, come, ever, recently,\n",
      "Nearest to and: or, but, thibetanus, one, circ, while, szko, dasyprocta,\n",
      "Nearest to which: that, this, what, it, also, but, usually, however,\n",
      "Nearest to while: although, when, however, but, though, before, circ, dasyprocta,\n",
      "Nearest to between: with, since, chiapas, into, around, plissken, circ, within,\n",
      "Nearest to he: she, it, they, there, who, never, eventually, bpm,\n",
      "Nearest to if: when, though, where, although, however, archaeopteryx, while, mitral,\n",
      "Nearest to to: would, will, wct, ursus, should, through, szko, landesverband,\n",
      "Nearest to were: are, was, have, had, those, be, is, been,\n",
      "Nearest to more: less, most, very, dominos, greater, rather, better, veitch,\n",
      "Nearest to people: men, children, those, landesverband, waw, women, zero, accounts,\n",
      "Nearest to six: five, seven, four, three, eight, nine, zero, hyi,\n",
      "Nearest to when: if, while, although, where, after, before, mamertines, though,\n",
      "Nearest to for: after, dasyprocta, during, busan, when, against, ensuring, wct,\n",
      "Average loss at step  220000 :  4.3289360687\n",
      "Average loss at step  240000 :  4.39369589608\n",
      "Average loss at step  260000 :  4.31544707125\n",
      "Average loss at step  280000 :  4.34802190154\n",
      "Average loss at step  300000 :  4.30021018691\n",
      "Average loss at step  320000 :  4.31524689076\n",
      "Average loss at step  340000 :  4.27723854162\n",
      "Average loss at step  360000 :  4.30084432677\n",
      "Average loss at step  380000 :  4.28915596345\n",
      "Average loss at step  400000 :  4.25223201886\n",
      "Nearest to from: through, into, during, in, rajons, under, mjw, within,\n",
      "Nearest to into: through, from, within, tempest, affectionate, across, elbl, between,\n",
      "Nearest to been: be, become, was, were, come, remained, recently, had,\n",
      "Nearest to and: or, but, including, pngimage, hyi, rajonas, wct, however,\n",
      "Nearest to which: that, this, what, usually, safl, typically, actually, but,\n",
      "Nearest to while: although, when, however, though, kinderhook, but, after, before,\n",
      "Nearest to between: with, lup, within, among, in, plissken, around, into,\n",
      "Nearest to he: she, it, they, there, who, we, waas, bpm,\n",
      "Nearest to if: when, though, where, although, since, archaeopteryx, while, however,\n",
      "Nearest to to: mjt, mjw, audita, wct, dasyprocta, ursus, agave, rajonas,\n",
      "Nearest to were: are, was, have, those, be, had, pontificia, rajonas,\n",
      "Nearest to more: less, most, very, quite, rather, greater, larger, longer,\n",
      "Nearest to people: men, women, children, jews, person, individuals, landesverband, them,\n",
      "Nearest to six: four, seven, five, eight, three, nine, zero, hyi,\n",
      "Nearest to when: if, while, although, where, before, after, though, during,\n",
      "Nearest to for: dasyprocta, wct, including, audita, mjw, in, rajons, if,\n",
      "Average loss at step  420000 :  4.2332284336\n",
      "Average loss at step  440000 :  4.26715067364\n",
      "Average loss at step  460000 :  4.28474414814\n",
      "Average loss at step  480000 :  4.1241690493\n",
      "Average loss at step  500000 :  4.25148263264\n",
      "Average loss at step  520000 :  4.18772170489\n",
      "Average loss at step  540000 :  4.22940573909\n",
      "Average loss at step  560000 :  4.20050674417\n",
      "Average loss at step  580000 :  4.20434691161\n",
      "Average loss at step  600000 :  4.19083545874\n",
      "Nearest to from: through, across, during, via, rajons, into, mjw, rajonas,\n",
      "Nearest to into: through, within, across, from, tempest, elbl, away, affectionate,\n",
      "Nearest to been: be, become, come, was, remained, already, were, recently,\n",
      "Nearest to and: or, but, however, rajonas, including, although, mammuthus, thibetanus,\n",
      "Nearest to which: that, this, what, actually, typically, safl, usually, rajonas,\n",
      "Nearest to while: although, when, though, however, kinderhook, including, unlike, after,\n",
      "Nearest to between: lup, with, among, within, across, plissken, hangs, mackaye,\n",
      "Nearest to he: she, it, they, there, we, who, waas, mitral,\n",
      "Nearest to if: when, though, where, although, since, before, unless, archaeopteryx,\n",
      "Nearest to to: audita, dasyprocta, mjw, mjt, wct, rajonas, would, agave,\n",
      "Nearest to were: are, have, was, those, these, pontificia, incline, although,\n",
      "Nearest to more: less, very, most, quite, greater, rather, larger, longer,\n",
      "Nearest to people: men, women, children, individuals, jews, peoples, fans, person,\n",
      "Nearest to six: four, five, eight, seven, three, two, nine, hyi,\n",
      "Nearest to when: if, while, although, where, before, though, after, however,\n",
      "Nearest to for: busan, dasyprocta, auditum, including, mjw, of, if, without,\n",
      "Average loss at step  620000 :  4.20978682024\n",
      "Average loss at step  640000 :  4.20852332526\n",
      "Average loss at step  660000 :  4.19281596824\n",
      "Average loss at step  680000 :  4.15824690018\n",
      "Average loss at step  700000 :  4.18065965886\n",
      "Average loss at step  720000 :  4.18245750785\n",
      "Average loss at step  740000 :  4.0688151175\n",
      "Average loss at step  760000 :  4.17679225191\n",
      "Average loss at step  780000 :  4.13362353218\n",
      "Average loss at step  800000 :  4.16932820062\n",
      "Nearest to from: through, in, into, via, rajons, during, across, mjw,\n",
      "Nearest to into: through, within, across, from, onto, tempest, away, davidic,\n",
      "Nearest to been: be, become, come, already, remained, recently, was, were,\n",
      "Nearest to and: or, but, however, including, rajonas, hyi, mammuthus, safl,\n",
      "Nearest to which: that, this, actually, what, typically, usually, islamists, safl,\n",
      "Nearest to while: although, when, though, however, began, unlike, kinderhook, after,\n",
      "Nearest to between: with, lup, within, among, across, hangs, blvd, inside,\n",
      "Nearest to he: she, it, they, there, we, who, waas, mitral,\n",
      "Nearest to if: when, though, although, where, since, unless, archaeopteryx, before,\n",
      "Nearest to to: mjt, hyi, wct, ursus, dasyprocta, mjw, agave, rajonas,\n",
      "Nearest to were: are, those, was, have, rajonas, these, incline, be,\n",
      "Nearest to more: less, most, quite, very, greater, enough, larger, rather,\n",
      "Nearest to people: men, women, individuals, jews, children, peoples, muslims, fans,\n",
      "Nearest to six: seven, eight, four, five, nine, three, hyi, landesverband,\n",
      "Nearest to when: if, while, although, before, where, though, after, however,\n",
      "Nearest to for: dasyprocta, rajons, wct, rajonas, after, mjw, in, busan,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  820000 :  4.13990458595\n",
      "Average loss at step  840000 :  4.14843492368\n",
      "Average loss at step  860000 :  4.16720801257\n",
      "Average loss at step  880000 :  4.11716017977\n",
      "Average loss at step  900000 :  4.16926217705\n",
      "Average loss at step  920000 :  4.14734866341\n",
      "Average loss at step  940000 :  4.12868653712\n",
      "Average loss at step  960000 :  4.14620680249\n",
      "Average loss at step  980000 :  4.13878755189\n",
      "Average loss at step  1000000 :  4.04580348073\n",
      "Nearest to from: through, via, rajons, during, across, besides, into, enhances,\n",
      "Nearest to into: through, within, across, onto, tempest, from, davidic, affectionate,\n",
      "Nearest to been: be, become, come, already, were, remained, was, recently,\n",
      "Nearest to and: but, rajonas, or, agave, thibetanus, dasyprocta, hyi, landesverband,\n",
      "Nearest to which: that, this, what, typically, actually, who, safl, but,\n",
      "Nearest to while: although, when, though, however, whereas, kinderhook, unlike, whilst,\n",
      "Nearest to between: lup, among, within, with, across, inside, blvd, marle,\n",
      "Nearest to he: she, it, they, there, we, who, mitral, whitlam,\n",
      "Nearest to if: when, though, unless, where, since, before, although, because,\n",
      "Nearest to to: hyi, mjt, mjw, dasyprocta, ursus, audita, wct, rajonas,\n",
      "Nearest to were: are, was, those, these, have, be, incline, rajonas,\n",
      "Nearest to more: less, quite, most, very, greater, worse, larger, enough,\n",
      "Nearest to people: men, women, individuals, jews, children, peoples, muslims, residents,\n",
      "Nearest to six: seven, eight, four, five, nine, three, zero, two,\n",
      "Nearest to when: if, while, although, where, before, after, unless, though,\n",
      "Nearest to for: dasyprocta, rajons, busan, rajonas, wct, audita, mjw, abandons,\n",
      "Average loss at step  1020000 :  4.10359834481\n",
      "Average loss at step  1040000 :  4.10447594263\n",
      "Average loss at step  1060000 :  4.10583353707\n",
      "Average loss at step  1080000 :  4.1073725705\n",
      "Average loss at step  1100000 :  4.12490047302\n",
      "Average loss at step  1120000 :  4.1265602081\n",
      "Average loss at step  1140000 :  4.09083834941\n",
      "Average loss at step  1160000 :  4.15155592268\n",
      "Average loss at step  1180000 :  4.10331000839\n",
      "Average loss at step  1200000 :  4.09355439812\n",
      "Nearest to from: through, via, besides, rajons, during, in, across, rajonas,\n",
      "Nearest to into: through, within, onto, across, tempest, from, davidic, satisfies,\n",
      "Nearest to been: be, become, already, remained, come, historically, were, previously,\n",
      "Nearest to and: but, or, however, nor, rajonas, thibetanus, including, although,\n",
      "Nearest to which: that, this, actually, typically, what, safl, who, islamists,\n",
      "Nearest to while: although, when, though, whilst, whereas, however, unlike, kinderhook,\n",
      "Nearest to between: with, lup, within, among, across, from, inside, over,\n",
      "Nearest to he: she, it, they, there, we, waas, who, mitral,\n",
      "Nearest to if: when, though, unless, where, since, although, hence, because,\n",
      "Nearest to to: mjt, rajonas, hyi, dasyprocta, mjw, wct, landesverband, audita,\n",
      "Nearest to were: are, was, have, those, these, rajonas, incline, pontificia,\n",
      "Nearest to more: less, quite, most, very, worse, larger, enough, greater,\n",
      "Nearest to people: men, women, individuals, jews, americans, peoples, muslims, citizens,\n",
      "Nearest to six: seven, five, four, eight, three, nine, two, hyi,\n",
      "Nearest to when: if, while, although, where, though, after, unless, before,\n",
      "Nearest to for: dasyprocta, rajons, rajonas, busan, hbox, mjw, audita, hyi,\n",
      "Average loss at step  1220000 :  4.084660001\n",
      "Average loss at step  1240000 :  4.10924338448\n",
      "Average loss at step  1260000 :  4.10428494163\n",
      "Average loss at step  1280000 :  3.97969400813\n",
      "Average loss at step  1300000 :  4.09817235332\n",
      "Average loss at step  1320000 :  4.03226239958\n",
      "Average loss at step  1340000 :  4.09316765916\n",
      "Average loss at step  1360000 :  4.07587454007\n",
      "Average loss at step  1380000 :  4.08070993\n",
      "Average loss at step  1400000 :  4.06950101672\n",
      "Nearest to from: via, through, rajons, besides, across, during, safl, mjw,\n",
      "Nearest to into: through, onto, within, across, davidic, tempest, from, beyond,\n",
      "Nearest to been: be, become, already, come, was, were, historically, previously,\n",
      "Nearest to and: or, but, nor, whereas, rajonas, kinderhook, including, while,\n",
      "Nearest to which: that, this, actually, typically, what, islamists, safl, but,\n",
      "Nearest to while: although, whereas, when, though, whilst, however, including, unlike,\n",
      "Nearest to between: lup, among, with, across, within, inside, shaku, over,\n",
      "Nearest to he: she, it, they, there, we, mitral, who, michelob,\n",
      "Nearest to if: when, though, unless, although, where, hence, because, whenever,\n",
      "Nearest to to: dasyprocta, rajonas, hyi, mjt, audita, mjw, wct, wideawake,\n",
      "Nearest to were: are, was, have, those, these, rajonas, incline, pontificia,\n",
      "Nearest to more: less, quite, most, very, greater, larger, enough, worse,\n",
      "Nearest to people: women, individuals, men, jews, fans, persons, americans, children,\n",
      "Nearest to six: eight, four, seven, five, three, nine, hyi, rajonas,\n",
      "Nearest to when: if, while, before, where, although, though, unless, after,\n",
      "Nearest to for: busan, dasyprocta, in, rajons, rajonas, audita, of, after,\n",
      "Average loss at step  1420000 :  4.09283684272\n",
      "Average loss at step  1440000 :  4.09022417452\n",
      "Average loss at step  1460000 :  4.0639601844\n",
      "Average loss at step  1480000 :  4.03760708206\n",
      "Average loss at step  1500000 :  4.09348774075\n",
      "Average loss at step  1520000 :  4.07851810901\n",
      "Average loss at step  1540000 :  3.9597240613\n",
      "Average loss at step  1560000 :  4.07499863696\n",
      "Average loss at step  1580000 :  4.01649859191\n",
      "Average loss at step  1600000 :  4.07773584781\n",
      "Nearest to from: via, through, besides, across, into, during, in, rajons,\n",
      "Nearest to into: onto, through, within, across, from, tempest, beyond, davidic,\n",
      "Nearest to been: be, become, already, come, fallen, grown, recently, previously,\n",
      "Nearest to and: but, rajonas, or, hyi, whereas, kinderhook, safl, although,\n",
      "Nearest to which: that, this, typically, actually, what, islamists, usually, rajonas,\n",
      "Nearest to while: although, whilst, when, whereas, though, however, after, began,\n",
      "Nearest to between: lup, among, with, across, within, inside, shaku, rajonas,\n",
      "Nearest to he: she, it, they, there, we, who, rohrabacher, mitral,\n",
      "Nearest to if: when, unless, though, where, although, hence, whenever, since,\n",
      "Nearest to to: dasyprocta, hyi, mjt, wct, rajonas, mjw, audita, landesverband,\n",
      "Nearest to were: are, was, those, have, these, incline, rajonas, remain,\n",
      "Nearest to more: less, quite, most, very, worse, fewer, greater, heavier,\n",
      "Nearest to people: individuals, women, men, jews, persons, peoples, citizens, children,\n",
      "Nearest to six: five, four, eight, seven, three, nine, zero, two,\n",
      "Nearest to when: if, although, while, where, though, before, after, unless,\n",
      "Nearest to for: dasyprocta, rajonas, if, rajons, hbox, audita, busan, despite,\n",
      "Average loss at step  1620000 :  4.04256130984\n",
      "Average loss at step  1640000 :  4.06324257388\n",
      "Average loss at step  1660000 :  4.04869626679\n",
      "Average loss at step  1680000 :  4.05765993543\n",
      "Average loss at step  1700000 :  4.08312831153\n",
      "Average loss at step  1720000 :  4.05981061043\n",
      "Average loss at step  1740000 :  4.02303984957\n",
      "Average loss at step  1760000 :  4.04922945347\n",
      "Average loss at step  1780000 :  4.0568610615\n",
      "Average loss at step  1800000 :  3.94806807138\n",
      "Nearest to from: via, through, besides, rajons, during, across, into, in,\n",
      "Nearest to into: through, onto, within, across, tempest, from, davidic, beyond,\n",
      "Nearest to been: be, become, already, fallen, come, were, grown, undergone,\n",
      "Nearest to and: but, or, rajonas, thibetanus, hyi, whereas, landesverband, one,\n",
      "Nearest to which: this, that, what, actually, safl, typically, instead, islamists,\n",
      "Nearest to while: although, whilst, whereas, when, though, despite, however, after,\n",
      "Nearest to between: lup, across, among, within, with, inside, shaku, tulare,\n",
      "Nearest to he: she, they, it, there, we, who, whitlam, michelob,\n",
      "Nearest to if: when, unless, though, although, where, whenever, because, hence,\n",
      "Nearest to to: hyi, rajonas, dasyprocta, mjt, mjw, wct, landesverband, microcebus,\n",
      "Nearest to were: are, was, those, these, incline, have, be, rajonas,\n",
      "Nearest to more: less, most, quite, very, worse, fewer, larger, heavier,\n",
      "Nearest to people: individuals, women, men, jews, persons, citizens, residents, tourists,\n",
      "Nearest to six: seven, four, eight, five, nine, three, zero, hyi,\n",
      "Nearest to when: if, while, although, unless, where, before, after, though,\n",
      "Nearest to for: after, busan, alongside, dasyprocta, rajons, during, despite, rajonas,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  1820000 :  4.0366222593\n",
      "Average loss at step  1840000 :  4.01050049885\n",
      "Average loss at step  1860000 :  4.04104860071\n",
      "Average loss at step  1880000 :  4.02129215995\n",
      "Average loss at step  1900000 :  4.03252021322\n",
      "Average loss at step  1920000 :  4.07884682775\n",
      "Average loss at step  1940000 :  4.00356332458\n",
      "Average loss at step  1960000 :  4.07981533021\n",
      "Average loss at step  1980000 :  4.03165701723\n",
      "Average loss at step  2000000 :  4.02683587987\n",
      "Nearest to from: via, besides, through, rajons, in, safl, during, handicap,\n",
      "Nearest to into: onto, through, within, across, davidic, tempest, from, beyond,\n",
      "Nearest to been: be, become, already, grown, fallen, come, historically, undergone,\n",
      "Nearest to and: but, nor, or, rajonas, however, dasyprocta, UNK, hyi,\n",
      "Nearest to which: that, this, typically, what, actually, but, safl, islamists,\n",
      "Nearest to while: although, whilst, whereas, when, though, however, despite, including,\n",
      "Nearest to between: lup, among, with, within, across, albertine, shaku, over,\n",
      "Nearest to he: she, they, it, there, we, rohrabacher, who, whitlam,\n",
      "Nearest to if: when, unless, though, where, hence, whenever, although, since,\n",
      "Nearest to to: rajonas, dasyprocta, audita, mjt, hyi, landesverband, mjw, for,\n",
      "Nearest to were: are, those, these, incline, was, have, rajonas, remain,\n",
      "Nearest to more: less, quite, fewer, most, worse, very, stronger, larger,\n",
      "Nearest to people: individuals, men, women, citizens, americans, jews, tourists, residents,\n",
      "Nearest to six: seven, five, eight, four, three, hyi, nine, rajonas,\n",
      "Nearest to when: if, while, where, although, unless, though, after, before,\n",
      "Nearest to for: rajonas, dasyprocta, rajons, if, busan, consider, to, after,\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Begin training.\n",
    "num_steps = 2000001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # We must initialize all variables before we use them.\n",
    "  init.run()\n",
    "  print('Initialized')\n",
    "\n",
    "  average_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch(\n",
    "        batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "    # We perform one update step by evaluating the optimizer op (including it\n",
    "    # in the list of returned values for session.run()\n",
    "    _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    if step % 20000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss /= 20000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print('Average loss at step ', step, ': ', average_loss)\n",
    "      average_loss = 0\n",
    "\n",
    "    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 200000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in range(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8  # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "        log_str = 'Nearest to %s:' % valid_word\n",
    "        for k in range(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log_str = '%s %s,' % (log_str, close_word)\n",
    "        print(log_str)\n",
    "  final_embeddings = normalized_embeddings.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 6: Visualize the embeddings.\n",
    "\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "  assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "  plt.figure(figsize=(18, 18))  # in inches\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = low_dim_embs[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "\n",
    "  plt.savefig(filename)\n",
    "\n",
    "try:\n",
    "  # pylint: disable=g-import-not-at-top\n",
    "  from sklearn.manifold import TSNE\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "  plot_only = 500\n",
    "  low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "  labels = [reverse_dictionary[i] for i in range(plot_only)]\n",
    "  plot_with_labels(low_dim_embs, labels)\n",
    "\n",
    "except ImportError:\n",
    "  print('Please install sklearn, matplotlib, and scipy to show embeddings.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
